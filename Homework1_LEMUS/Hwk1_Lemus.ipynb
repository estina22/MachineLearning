{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from keras.utils import np_utils\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import scipy\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/eslemus/Desktop/ML/anticl/'\n",
    "\n",
    "img_size = 150\n",
    "im_l =240\n",
    "im_b =320\n",
    "# all parameters not specified are set to their defaults\n",
    "\n",
    "logisticRegr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder):\n",
    "    \"\"\"\n",
    "    Load the data and labels from the given folder.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seismic_type in os.listdir(folder):\n",
    "        if not seismic_type.startswith('.'):\n",
    "            if seismic_type in ['Class1']:\n",
    "                label = '0'\n",
    "            else:\n",
    "                label = '1'\n",
    "            for image_filename in os.listdir(folder + seismic_type):\n",
    "                img_file = cv2.imread(folder + seismic_type + '/' + image_filename, 0)\n",
    "                if img_file is not None:\n",
    "                    # Downsample the image to 120, 160, 3\n",
    "                    # img_file = scipy.misc.imresize(arr=img_file, size=(120, 160, 3))\n",
    "                    img_arr = np.asarray(img_file)\n",
    "                    X.append(img_arr)\n",
    "                    y.append(label)\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_data(BASE_DIR + 'images/Train/')\n",
    "X_test, y_test = get_data(BASE_DIR + 'images/Test/')\n",
    "\n",
    "#scikit-learn expects 2d num arrays for the training dataset for a fit function. \n",
    "#The dataset you are passing in is a 3d array you need to reshape the array into a 2d.\n",
    "\n",
    "nsamples, nx, ny = X_train.shape\n",
    "nsamples2, nx2, ny2 = X_test.shape\n",
    "X_train_new = X_train.reshape((nsamples,nx*ny))\n",
    "X_test = X_test.reshape((nsamples2,nx2*ny2))\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr.fit(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "print('Predicting on test data')\n",
    "y_pred = np.rint(logisticRegr.predict(X_test))\n",
    "\n",
    "#predictions = logisticRegr.predict(x_test)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(score)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search for model tuning A model hyperparameter is a characteristic of a model that is external to the model and whose value cannot be estimated from data. The value of the hyperparameter has to be set before the learning process begins. Grid-search is used to find the optimal hyperparameters of a model which results in the most ‘accurate’ predictions. The hyperparameters we tuned are: 1) Penalty: l1 or l2 which species the norm used in the penalization. 2) C: Inverse of regularization strength- smaller values of C specify stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "logisticRegr = LogisticRegression()\n",
    "grid_values = {'penalty': ['l1', 'l2'],'C':[0.001,.009,0.01,.09,1,5,10,25]}\n",
    "grid_LR_acc = GridSearchCV(logisticRegr, param_grid = grid_values, scoring = None)\n",
    "grid_LR_acc.fit(X_train_new, y_train)\n",
    "\n",
    "#Predict values based on new parameters\n",
    "y_pred_acc = grid_LR_acc.predict(X_test)\n",
    "\n",
    "# New Model Evaluation metrics \n",
    "print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred_acc)))\n",
    "print('Precision Score : ' + str(precision_score(y_test,y_pred_acc, average = None)))\n",
    "print('Recall Score : ' + str(recall_score(y_test,y_pred_acc, average = None)))\n",
    "print('F1 Score : ' + str(f1_score(y_test,y_pred_acc, average = None)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------KNN Classifier \n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "#try KNN for diffrent k nearest neighbor from 1 to 10\n",
    "neighbors_setting = range(1,10)\n",
    "\n",
    "for n_neighbors in neighbors_setting:\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train_new, y_train)\n",
    "    training_accuracy.append(knn.score(X_train_new, y_train))\n",
    "    test_accuracy.append(knn.score(X_test, y_test))\n",
    " \n",
    "plt.plot(neighbors_setting,training_accuracy, label='Accuracy of the training set')\n",
    "plt.plot(neighbors_setting,test_accuracy, label='Accuracy of the test set')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#by looking at plot, best result accurs when n_neighbors is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of the training set for 3NN: {:3f}\".format(training_accuracy[3]))\n",
    "print(\"Accuracy of the test set for 3NN: {:3f}\".format(test_accuracy[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnop = KNeighborsClassifier(n_neighbors=3)\n",
    "knnop.fit(X_train_new, y_train)\n",
    "predictionsknn = knnop.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, predictionsknn))\n",
    "print(confusion_matrix(y_test, predictionsknn))\n",
    "print(classification_report(y_test, predictionsknn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title, size = 8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: The most crucial aspect of algorithm training within Machine Learning is having a dependable dataset. There exists several processes for enhancing predictions and accuracy. In this case, we used Logistic Regression with parameter optimization and then an implementation of KNN. For the classification of seismic images, this specific python notebook needs a larger dataset than the one provided. -Estefany Lemus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
